{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPk0K4aiSwnmH/Vd+eT3EQo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report"],"metadata":{"id":"-mrPjWhLRfrd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Load the Titanic dataset\n","url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n","titanic = pd.read_csv(url)\n"],"metadata":{"id":"O_TN3HKsRfpM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Exploring missing values\n","titanic.isnull().sum()"],"metadata":{"id":"qanDVruQRfm2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Visualize missing values\n","plt.figure(figsize=(10, 6))\n","sns.heatmap(titanic.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n","plt.title('Missing Value Heatmap')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"_UqaFVG9RfkQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate percentage of missing values per column\n","missing_percentage = titanic.isnull().mean() * 100\n","missing_percentage = missing_percentage[missing_percentage > 0].sort_values(ascending=False)\n","plt.figure(figsize=(10, 6))\n","missing_percentage.plot(kind='bar')\n","plt.title('Percentage of Missing Values by Column')\n","plt.ylabel('Percentage')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"qTy83KRsRfhp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate percentage of missing values per column\n","missing_percentage = titanic.isnull().mean() * 100\n","missing_percentage = missing_percentage[missing_percentage > 0].sort_values(ascending=False)\n","plt.figure(figsize=(10, 6))\n","missing_percentage.plot(kind='bar')\n","plt.title('Percentage of Missing Values by Column')\n","plt.ylabel('Percentage')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"QJbqdj5cRffg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"L2ZNbTIOSHqN"}},{"cell_type":"markdown","source":["# HANDLING MISSING VALUES"],"metadata":{"id":"0rXYZ6snSJ46"}},{"cell_type":"code","source":["# 1. Deletion approach - not always recommended but useful for some columns\n","titanic_reduced = titanic.drop(['Cabin', 'Ticket'], axis=1)  # High missingness or less relevant"],"metadata":{"id":"FQxNru8RRfaN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. Simple imputation for Age (using median grouped by Pclass)\n","age_imputed = titanic_reduced.copy()\n","age_imputed['Age'] = age_imputed.groupby('Pclass')['Age'].transform(lambda x: x.fillna(x.median()))"],"metadata":{"id":"H9uf4oioRfYB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# 3. Create missing indicator for Age\n","age_imputed['Age_Missing'] = titanic_reduced['Age'].isnull().astype(int)"],"metadata":{"id":"hQEFxho6RfVe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4. Mode imputation for categorical column (Embarked)\n","age_imputed['Embarked'] = age_imputed['Embarked'].fillna(age_imputed['Embarked'].mode()[0])"],"metadata":{"id":"W9c_S0L9RfS9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify imputation results\n","age_imputed.isnull().sum()"],"metadata":{"id":"4udm6whORfQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize Age distribution before and after imputation\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","sns.histplot(titanic['Age'].dropna(), kde=True)\n","plt.title('Original Age Distribution')"],"metadata":{"id":"0CxB5TBaRfNf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.subplot(1, 2, 2)\n","sns.histplot(age_imputed['Age'], kde=True)\n","plt.title('Age After Imputation')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"a6S-3ro2RfG6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# FEATURE SCALING"],"metadata":{"id":"dCSJk2s_SZbJ"}},{"cell_type":"code","source":["# Select relevant columns and prepare a dataset for ML\n","features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n","target = 'Survived'\n","X = age_imputed[features]\n","y = age_imputed[target]"],"metadata":{"id":"Fcj37cKMSYZZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"],"metadata":{"id":"oMkoPgzySYWV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Standardization (z-score normalization)\n","scaler = StandardScaler()\n","# Never directly transform your entire dataset - only fit on training data\n","numeric_features = ['Age', 'Fare']\n","X_train_std = X_train.copy()\n","X_test_std = X_test.copy()\n","\n","X_train_std[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n","X_test_std[numeric_features] = scaler.transform(X_test[numeric_features])"],"metadata":{"id":"Um4WspAkSYT4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. Min-Max Scaling (normalization)\n","min_max_scaler = MinMaxScaler()\n","X_train_norm = X_train.copy()\n","X_test_norm = X_test.copy()\n","\n","X_train_norm[numeric_features] = min_max_scaler.fit_transform(X_train[numeric_features])\n","X_test_norm[numeric_features] = min_max_scaler.transform(X_test[numeric_features])"],"metadata":{"id":"vc_eUe8BSYRU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize scaling effects\n","plt.figure(figsize=(15, 5))\n","plt.subplot(1, 3, 1)\n","sns.kdeplot(X_train['Fare'], label='Original')\n","plt.title('Original Fare Distribution')"],"metadata":{"id":"VCpg8le7SYPX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.subplot(1, 3, 2)\n","sns.kdeplot(X_train_std['Fare'], label='Standardized')\n","plt.title('Standardized Fare')"],"metadata":{"id":"qpCiuwkESYM-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.subplot(1, 3, 3)\n","sns.kdeplot(X_train_norm['Fare'], label='Normalized')\n","plt.title('Normalized Fare')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"UgAPtEQsSYKH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ENCODING CATEGORICAL VARIABLES"],"metadata":{"id":"JWehOHZ9SrxW"}},{"cell_type":"code","source":["# 1. Label Encoding for ordinal features\n","label_encoder = LabelEncoder()\n","X_train_encoded = X_train_std.copy()\n","X_test_encoded = X_test_std.copy()"],"metadata":{"id":"z404cl4fSrny"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply label encoding for Sex column\n","X_train_encoded['Sex'] = label_encoder.fit_transform(X_train_encoded['Sex'])\n","X_test_encoded['Sex'] = label_encoder.transform(X_test_encoded['Sex'])"],"metadata":{"id":"1HEjpVfmSrlW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# 2. One-Hot Encoding for nominal features\n","# Creating a one-hot encoder for 'Embarked'\n","embarked_ohe = OneHotEncoder(sparse_output=False, drop='first')\n","# Fit on training data\n","embarked_train_encoded = embarked_ohe.fit_transform(X_train_encoded[['Embarked']])\n","embarked_test_encoded = embarked_ohe.transform(X_test_encoded[['Embarked']])"],"metadata":{"id":"qkQ3abY0Srg9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create dataframes with the encoded columns\n","embarked_train_df = pd.DataFrame(\n","    embarked_train_encoded,\n","    columns=[f'Embarked_{c}' for c in embarked_ohe.categories_[0][1:]],\n","    index=X_train_encoded.index\n",")\n","embarked_test_df = pd.DataFrame(\n","    embarked_test_encoded,\n","    columns=[f'Embarked_{c}' for c in embarked_ohe.categories_[0][1:]],\n","    index=X_test_encoded.index\n",")"],"metadata":{"id":"HS157FjsSreP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Drop original Embarked column and join encoded columns\n","X_train_encoded = X_train_encoded.drop('Embarked', axis=1).join(embarked_train_df)\n","X_test_encoded = X_test_encoded.drop('Embarked', axis=1).join(embarked_test_df)\n"],"metadata":{"id":"Ou8COp1xSYHV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compare encoding methods\n","print(\"Label Encoded 'Sex':\")\n","print(X_train_encoded['Sex'].value_counts())"],"metadata":{"id":"JLTN5UqKS1g4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\nOne-Hot Encoded 'Embarked':\")\n","print(X_train_encoded.filter(like='Embarked').head())"],"metadata":{"id":"Muf4ohSsS1cT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# BUILDING PREPROCESSING PIPELINES"],"metadata":{"id":"JUe3WIzXS42-"}},{"cell_type":"code","source":["# Define preprocessing for numerical columns\n","numerical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler', StandardScaler())\n","])"],"metadata":{"id":"cdTOdyAfS1Z6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define preprocessing for categorical columns\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='most_frequent')),\n","    ('onehot', OneHotEncoder(drop='first'))\n","])"],"metadata":{"id":"-HhtyVl9S1XT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combine preprocessing steps\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numerical_transformer, numeric_features),\n","        ('cat', categorical_transformer, ['Sex', 'Embarked'])\n","    ])\n"],"metadata":{"id":"S82y6RtKS1Uq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a full pipeline with preprocessing and model\n","model_pipeline = Pipeline(steps=[\n","    ('preprocessor', preprocessor),\n","    ('classifier', LogisticRegression())\n","])"],"metadata":{"id":"4FlKsby2S_rC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reset to use raw features for complete pipeline demonstration\n","X = age_imputed[features]\n","y = age_imputed[target]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"],"metadata":{"id":"VKsThsZxS_ox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the pipeline\n","model_pipeline.fit(X_train, y_train)"],"metadata":{"id":"9Xels16yS_lz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make predictions\n","predictions = model_pipeline.predict(X_test)"],"metadata":{"id":"9jJD6M5vS_jH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model\n","print(f\"Accuracy: {accuracy_score(y_test, predictions):.4f}\")\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, predictions))"],"metadata":{"id":"oI5eAMNeS1SN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ADVANCED: Custom Pipeline with Cross-Validation"],"metadata":{"id":"TnrSsC9kTHbu"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","# Create a pipeline with multiple possible models\n","pipeline = Pipeline([\n","    ('preprocessor', preprocessor),\n","    ('classifier', RandomForestClassifier())\n","])"],"metadata":{"id":"5yplb_SRS1Pu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define parameters to search\n","param_grid = {\n","    'classifier__n_estimators': [50, 100],\n","    'classifier__max_depth': [None, 10],\n","    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n","}"],"metadata":{"id":"aC-GDj8vTFpp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up GridSearchCV\n","grid_search = GridSearchCV(pipeline, param_grid, cv=5,\n","                          scoring='accuracy', n_jobs=-1)"],"metadata":{"id":"PCsIOdPGTFnY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fit the grid search\n","grid_search.fit(X_train, y_train)\n"],"metadata":{"id":"nzRl1p1zTFk8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Best parameters and score\n","print(f\"Best parameters: {grid_search.best_params_}\")\n","print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"],"metadata":{"id":"ubF5SUm1TFiZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate on test set\n","test_score = grid_search.score(X_test, y_test)\n","print(f\"Test set score with best parameters: {test_score:.4f}\")"],"metadata":{"id":"F8sN-65fTFf0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BONUS: Feature importance from the best model\n","if hasattr(grid_search.best_estimator_.named_steps['classifier'], 'feature_importances_'):\n","    # Get the preprocessor\n","    preprocessor = grid_search.best_estimator_.named_steps['preprocessor']\n","\n","    # Get the feature names after transformation\n","    ohe_features = preprocessor.transformers_[1][1].named_steps['onehot'].get_feature_names_out(['Sex', 'Embarked'])\n","    feature_names = np.concatenate([numeric_features, ohe_features])\n","\n","    # Get feature importances\n","    importances = grid_search.best_estimator_.named_steps['classifier'].feature_importances_\n","\n","    # Create a DataFrame for visualization\n","    importance_df = pd.DataFrame({\n","        'Feature': feature_names,\n","        'Importance': importances\n","    }).sort_values('Importance', ascending=False)\n","\n","    # Plot feature importances\n","    plt.figure(figsize=(12, 6))\n","    sns.barplot(x='Importance', y='Feature', data=importance_df)\n","    plt.title('Feature Importances from Best Random Forest Model')\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"Twbx2h2XTPpu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Excercise"],"metadata":{"id":"DpBjSCyLAUZl"}},{"cell_type":"markdown","source":["# Week 3 Day 1: Titanic Data Preprocessing & Modeling\n","\n","In this exercise you'll reinforce:\n","\n","- **Missing‑value exploration & visualization**  \n","- **Imputation strategies** (deletion, group‑median, mode, missing indicator)  \n","- **Feature scaling** (StandardScaler vs MinMaxScaler)  \n","- **Categorical encoding** (LabelEncoder, OneHotEncoder)  \n","- **Building end‑to‑end pipelines** with `ColumnTransformer` + `Pipeline`  \n","- **Model training & evaluation** (LogisticRegression, RandomForest)  \n","- **Hyperparameter tuning** via `GridSearchCV`  \n","- **Feature importance** extraction  \n","\n","> **Instructions:**  \n","> 1. Don’t modify cells above the first `# TODO`.  \n","> 2. Replace each `# TODO` with your code.  \n","> 3. Run cells sequentially and verify each output.  \n","> 4. Wherever requested, add a brief comment on what you observe."],"metadata":{"id":"zPiDSeY3HYFs"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# 1.1 Load dataset\n","url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n","titanic = pd.read_csv(url)\n","\n","# 1.2 How many missing values per column?\n","# TODO: Compute and display titanic.isnull().sum()\n","\n","# 1.3 Visualize missingness with a heatmap\n","plt.figure(figsize=(10,6))\n","# TODO: use sns.heatmap(...) to show where data is missing\n","\n","# 1.4 Percentage of missing values by column (bar plot)\n","# TODO: calculate percentage and plot as a bar chart\n"],"metadata":{"id":"k1bWpvFlFX1b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2- Handle Missing Values ---"],"metadata":{"id":"XeXQaZVQHq4Y"}},{"cell_type":"code","source":["# Start from a copy\n","df = titanic.copy()\n","\n","# 2.1 Drop columns with too many missing or irrelevant\n","# TODO: df = df.drop([...], axis=1)\n","\n","# 2.2 Impute Age by median within each Pclass\n","# TODO: df['Age'] = df.groupby('Pclass')['Age'].transform(...)\n","\n","# 2.3 Create Age_missing indicator column\n","# TODO: df['Age_Missing'] = ...\n","\n","# 2.4 Impute Embarked with mode\n","# TODO: df['Embarked'] = df['Embarked'].fillna(...)\n","\n","# 2.5 Verify no more missing values\n","# TODO: df.isnull().sum()\n"],"metadata":{"id":"eRKp_GxVFXy0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Feature Scaling"],"metadata":{"id":"TjMR23rEHn0Y"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","\n","features = ['Age','Fare']\n","X = df[features]\n","y = df['Survived']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# 3.1 Standardization\n","std = StandardScaler()\n","# TODO: fit on X_train, transform both X_train and X_test\n","\n","# 3.2 Min‑Max normalization\n","mms = MinMaxScaler()\n","# TODO: fit on X_train, transform both X_train and X_test\n","\n","# 3.3 Plot distributions: original vs standardized vs normalized\n","plt.figure(figsize=(15,5))\n","# TODO: three subplots with sns.kdeplot for Fare in each dataset\n"],"metadata":{"id":"DdPGMGKEFXwK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Encode Categorical Features"],"metadata":{"id":"iphoi6SeHwyu"}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","\n","X_cat = df[['Sex','Embarked']]\n","\n","# 4.1 Label‑encode Sex\n","le = LabelEncoder()\n","# TODO: X_cat['Sex_le'] = ...\n","\n","# 4.2 One‑hot encode Embarked (drop first)\n","ohe = OneHotEncoder(sparse_output=False, drop='first')\n","# TODO: fit_transform Embarked and create a DataFrame with new columns\n","\n","# Display your encoded features\n","# TODO: print head of resulting DataFrame\n"],"metadata":{"id":"U-fNq30vHuIf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. Build a Preprocessing + Model Pipeline\n","\n"],"metadata":{"id":"FI_XqI_fH0af"}},{"cell_type":"code","source":["from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# 5.1 Define transformers\n","num_features = ['Age','Fare']\n","cat_features = ['Sex','Embarked']\n","\n","num_transformer = Pipeline([\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler', StandardScaler())\n","])\n","\n","cat_transformer = Pipeline([\n","    ('imputer', SimpleImputer(strategy='most_frequent')),\n","    ('onehot', OneHotEncoder(drop='first'))\n","])\n","\n","preprocessor = ColumnTransformer([\n","    ('num', num_transformer, num_features),\n","    ('cat', cat_transformer, cat_features)\n","])\n","\n","# 5.2 Create full pipeline with Logistic Regression\n","pipe = Pipeline([\n","    ('preproc', preprocessor),\n","    ('clf', LogisticRegression(max_iter=1000))\n","])\n","\n","# 5.3 Split raw data and train\n","X_full = df[num_features + cat_features]\n","y_full = df['Survived']\n","X_tr, X_te, y_tr, y_te = train_test_split(X_full, y_full, test_size=0.3, random_state=42)\n","\n","# TODO: fit pipe on X_tr, y_tr\n","# TODO: predict on X_te, compute accuracy and print classification_report\n"],"metadata":{"id":"6C9FoHwqFXtS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6. Hyperparameter Tuning with GridSearchCV"],"metadata":{"id":"beS453TuH9wV"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","# 6.1 Replace classifier in pipeline\n","tune_pipe = Pipeline([\n","    ('preproc', preprocessor),\n","    ('clf', RandomForestClassifier(random_state=42))\n","])\n","\n","# 6.2 Set up parameter grid\n","param_grid = {\n","    'clf__n_estimators': [50, 100],\n","    'clf__max_depth': [None, 10],\n","    'preproc__num__imputer__strategy': ['mean','median']\n","}\n","\n","grid = GridSearchCV(tune_pipe, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n","\n","# TODO: fit grid on X_tr, y_tr\n","# TODO: print best_params_ and best_score_\n","\n","# 6.3 Evaluate best estimator on test set\n","# TODO: grid.best_estimator_.score(X_te, y_te)\n","\n","# 6.4 (Bonus) If RandomForest gives feature_importances_, extract and plot the top 10\n","# Hint: use .named_steps['preproc'] to get transformer and get_feature_names_out\n"],"metadata":{"id":"AUPEEd9eAnBD"},"execution_count":null,"outputs":[]}]}